{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10c9891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:05:44.146328Z",
     "iopub.status.busy": "2024-06-12T06:05:44.145905Z",
     "iopub.status.idle": "2024-06-12T06:06:04.555843Z",
     "shell.execute_reply": "2024-06-12T06:06:04.553984Z"
    },
    "papermill": {
     "duration": 20.4192,
     "end_time": "2024-06-12T06:06:04.559784",
     "exception": false,
     "start_time": "2024-06-12T06:05:44.140584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 06:05:50.158109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 06:05:50.158471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 06:05:50.348643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV file...\n",
      "CSV file loaded.\n",
      "First few rows of the dataframe:\n",
      "        Date   Time  User1 ID  User2 ID  \\\n",
      "0   3/4/2021  10:17       1.0       2.0   \n",
      "1  1/24/2022   4:27       1.0       2.0   \n",
      "2   2/3/2021   2:19       1.0       2.0   \n",
      "3   9/4/2021   8:54       1.0       2.0   \n",
      "4  7/21/2021  13:15       1.0       2.0   \n",
      "\n",
      "                                             Message  Label  Unnamed: 6  \\\n",
      "0  bye bye dear bajaj  i got some better work to ...    1.0         NaN   \n",
      "1  Haha your so funny you sit on wikipedia all da...    1.0         NaN   \n",
      "2  My problem is people talking out of their asse...    1.0         NaN   \n",
      "3                                    Article updated    0.0         NaN   \n",
      "4  Well arent you phucking special Its easy to ha...    1.0         NaN   \n",
      "\n",
      "   Unnamed: 7  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "Extracting features and target variable...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU,Embedding, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "print(\"Loading CSV file...\")\n",
    "file_path = '/kaggle/input/data-ver-4/5. Communication_Data_Among_Users punya br.csv'  # Adjust the path as necessary\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"CSV file loaded.\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"First few rows of the dataframe:\")\n",
    "print(data.head())\n",
    "\n",
    "# Extract features and target variable\n",
    "print(\"Extracting features and target variable...\")\n",
    "X = data['Message'].astype(str)\n",
    "y = data['Label']\n",
    "\n",
    "# Handle missing values by filling NaNs with the most frequent value in target\n",
    "y = y.fillna(y.mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7769bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:06:04.570195Z",
     "iopub.status.busy": "2024-06-12T06:06:04.568639Z",
     "iopub.status.idle": "2024-06-12T06:06:07.145525Z",
     "shell.execute_reply": "2024-06-12T06:06:07.143814Z"
    },
    "papermill": {
     "duration": 2.585575,
     "end_time": "2024-06-12T06:06:07.149308",
     "exception": false,
     "start_time": "2024-06-12T06:06:04.563733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text data to TF-IDF features...\n",
      "TF-IDF conversion completed.\n",
      "Training set size (TF-IDF): 32483\n",
      "Test set size (TF-IDF): 8121\n"
     ]
    }
   ],
   "source": [
    "# Convert text data to TF-IDF features\n",
    "print(\"Converting text data to TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=30)  # Set max_features to 30\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n",
    "print(\"TF-IDF conversion completed.\")\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size (TF-IDF): {X_train_tfidf.shape[0]}\")\n",
    "print(f\"Test set size (TF-IDF): {X_test_tfidf.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36aff29e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:06:07.160512Z",
     "iopub.status.busy": "2024-06-12T06:06:07.159143Z",
     "iopub.status.idle": "2024-06-12T06:06:11.649122Z",
     "shell.execute_reply": "2024-06-12T06:06:11.647905Z"
    },
    "papermill": {
     "duration": 4.498162,
     "end_time": "2024-06-12T06:06:11.651828",
     "exception": false,
     "start_time": "2024-06-12T06:06:07.153666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the text data...\n",
      "Padding sequences...\n",
      "Sequences padded.\n",
      "Training set size (Sequences): 32483\n",
      "Test set size (Sequences): 8121\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "print(\"Tokenizing the text data...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "print(\"Padding sequences...\")\n",
    "max_sequence_length = 100  # Limit sequence length to 100 tokens\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "print(\"Sequences padded.\")\n",
    "\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size (Sequences): {X_train_seq.shape[0]}\")\n",
    "print(f\"Test set size (Sequences): {X_test_seq.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d5e348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:06:11.661732Z",
     "iopub.status.busy": "2024-06-12T06:06:11.661320Z",
     "iopub.status.idle": "2024-06-12T06:06:22.474924Z",
     "shell.execute_reply": "2024-06-12T06:06:22.473019Z"
    },
    "papermill": {
     "duration": 10.821662,
     "end_time": "2024-06-12T06:06:22.477721",
     "exception": false,
     "start_time": "2024-06-12T06:06:11.656059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model...\n",
      "Random Forest model trained.\n",
      "Making predictions with Random Forest model...\n",
      "Random Forest model predictions completed.\n",
      "Evaluating the Random Forest model...\n",
      "Random Forest Accuracy: 0.7141977588966876\n",
      "Random Forest Precision: 0.7066427289048474\n",
      "Random Forest Recall: 0.5668202764976958\n",
      "Random Forest F1 Score: 0.6290554578871662\n",
      "Random Forest Confusion Matrix:\n",
      "[[3832  817]\n",
      " [1504 1968]]\n"
     ]
    }
   ],
   "source": [
    "# Train the RandomForest model\n",
    "print(\"Training the Random Forest model...\")\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "print(\"Random Forest model trained.\")\n",
    "\n",
    "# Make predictions with RandomForest model\n",
    "print(\"Making predictions with Random Forest model...\")\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(\"Random Forest model predictions completed.\")\n",
    "\n",
    "# Evaluate the RandomForest model\n",
    "print(\"Evaluating the Random Forest model...\")\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
    "print(f'Random Forest Precision: {precision_rf}')\n",
    "print(f'Random Forest Recall: {recall_rf}')\n",
    "print(f'Random Forest F1 Score: {f1_rf}')\n",
    "print('Random Forest Confusion Matrix:')\n",
    "print(conf_matrix_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf40434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:06:22.487929Z",
     "iopub.status.busy": "2024-06-12T06:06:22.487432Z",
     "iopub.status.idle": "2024-06-12T06:08:23.681551Z",
     "shell.execute_reply": "2024-06-12T06:08:23.680093Z"
    },
    "papermill": {
     "duration": 121.203249,
     "end_time": "2024-06-12T06:08:23.685028",
     "exception": false,
     "start_time": "2024-06-12T06:06:22.481779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SVM model...\n",
      "SVM model trained.\n",
      "Making predictions with SVM model...\n",
      "SVM model predictions completed.\n",
      "Evaluating the SVM model...\n",
      "SVM Accuracy: 0.7162910971555227\n",
      "SVM Precision: 0.7279469164715067\n",
      "SVM Recall: 0.5371543778801844\n",
      "SVM F1 Score: 0.6181637388133908\n",
      "SVM Confusion Matrix:\n",
      "[[3952  697]\n",
      " [1607 1865]]\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "print(\"Training the SVM model...\")\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "print(\"SVM model trained.\")\n",
    "\n",
    "# Make predictions with SVM model\n",
    "print(\"Making predictions with SVM model...\")\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"SVM model predictions completed.\")\n",
    "\n",
    "# Evaluate the SVM model\n",
    "print(\"Evaluating the SVM model...\")\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "print(f'SVM Accuracy: {accuracy_svm}')\n",
    "print(f'SVM Precision: {precision_svm}')\n",
    "print(f'SVM Recall: {recall_svm}')\n",
    "print(f'SVM F1 Score: {f1_svm}')\n",
    "print('SVM Confusion Matrix:')\n",
    "print(conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96104ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:08:23.695371Z",
     "iopub.status.busy": "2024-06-12T06:08:23.695000Z",
     "iopub.status.idle": "2024-06-12T06:09:10.900122Z",
     "shell.execute_reply": "2024-06-12T06:09:10.898844Z"
    },
    "papermill": {
     "duration": 47.213229,
     "end_time": "2024-06-12T06:09:10.902676",
     "exception": false,
     "start_time": "2024-06-12T06:08:23.689447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the optimized CNN model...\n",
      "Compiling the CNN model...\n",
      "Training the CNN model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.6932 - loss: 0.5559 - val_accuracy: 0.8602 - val_loss: 0.3285\n",
      "Epoch 2/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.8945 - loss: 0.2629 - val_accuracy: 0.8585 - val_loss: 0.3639\n",
      "Epoch 3/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9472 - loss: 0.1476 - val_accuracy: 0.8533 - val_loss: 0.4632\n",
      "Epoch 4/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9774 - loss: 0.0714 - val_accuracy: 0.8493 - val_loss: 0.6176\n",
      "CNN model training completed.\n",
      "Evaluating the CNN model...\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8491 - loss: 0.6268\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "CNN Accuracy: 0.8492796421051025\n",
      "CNN Precision: 0.8600256245996156\n",
      "CNN Recall: 0.7733294930875576\n",
      "CNN F1 Score: 0.8143767060964513\n",
      "CNN Confusion Matrix:\n",
      "[[4212  437]\n",
      " [ 787 2685]]\n"
     ]
    }
   ],
   "source": [
    "# Create TensorFlow datasets for better memory management\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_seq, y_train_seq)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_seq, y_test_seq)).batch(64)\n",
    "\n",
    "# Define and train the optimized CNN model\n",
    "print(\"Defining the optimized CNN model...\")\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_sequence_length))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(16, activation='relu'))\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the CNN model\n",
    "print(\"Compiling the CNN model...\")\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the CNN model\n",
    "print(\"Training the CNN model...\")\n",
    "cnn_model.fit(train_dataset, epochs=10, validation_data=test_dataset, callbacks=[early_stopping])\n",
    "print(\"CNN model training completed.\")\n",
    "\n",
    "# Evaluate the CNN model\n",
    "print(\"Evaluating the CNN model...\")\n",
    "loss_cnn, accuracy_cnn = cnn_model.evaluate(test_dataset)\n",
    "y_pred_cnn_probs = cnn_model.predict(test_dataset)\n",
    "y_pred_cnn = (y_pred_cnn_probs > 0.5).astype(int)\n",
    "\n",
    "# Collect predictions for calculating metrics\n",
    "y_test_array = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_pred_cnn_array = np.concatenate(y_pred_cnn, axis=0)\n",
    "\n",
    "precision_cnn = precision_score(y_test_array, y_pred_cnn_array)\n",
    "recall_cnn = recall_score(y_test_array, y_pred_cnn_array)\n",
    "f1_cnn = f1_score(y_test_array, y_pred_cnn_array)\n",
    "conf_matrix_cnn = confusion_matrix(y_test_array, y_pred_cnn_array)\n",
    "\n",
    "print(f'CNN Accuracy: {accuracy_cnn}')\n",
    "print(f'CNN Precision: {precision_cnn}')\n",
    "print(f'CNN Recall: {recall_cnn}')\n",
    "print(f'CNN F1 Score: {f1_cnn}')\n",
    "print('CNN Confusion Matrix:')\n",
    "print(conf_matrix_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c356c2bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:09:11.013401Z",
     "iopub.status.busy": "2024-06-12T06:09:11.012997Z",
     "iopub.status.idle": "2024-06-12T06:13:53.053106Z",
     "shell.execute_reply": "2024-06-12T06:13:53.050997Z"
    },
    "papermill": {
     "duration": 282.100694,
     "end_time": "2024-06-12T06:13:53.057135",
     "exception": false,
     "start_time": "2024-06-12T06:09:10.956441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the optimized RNN model...\n",
      "Compiling the RNN model...\n",
      "Training the RNN model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 131ms/step - accuracy: 0.7295 - loss: 0.5058 - val_accuracy: 0.8676 - val_loss: 0.3269\n",
      "Epoch 2/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 132ms/step - accuracy: 0.9011 - loss: 0.2475 - val_accuracy: 0.8674 - val_loss: 0.3409\n",
      "Epoch 3/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 132ms/step - accuracy: 0.9434 - loss: 0.1579 - val_accuracy: 0.8552 - val_loss: 0.4292\n",
      "Epoch 4/10\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 132ms/step - accuracy: 0.9650 - loss: 0.1031 - val_accuracy: 0.8431 - val_loss: 0.5036\n",
      "RNN model training completed.\n",
      "Evaluating the RNN model...\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.8438 - loss: 0.5161\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step\n",
      "RNN Accuracy: 0.8431227803230286\n",
      "RNN Precision: 0.8118615209988649\n",
      "RNN Recall: 0.824020737327189\n",
      "RNN F1 Score: 0.8178959405374499\n",
      "RNN Confusion Matrix:\n",
      "[[3986  663]\n",
      " [ 611 2861]]\n"
     ]
    }
   ],
   "source": [
    "# Create TensorFlow datasets for better memory management\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_seq, y_train_seq)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_seq, y_test_seq)).batch(64)\n",
    "# Define and train the optimized RNN model\n",
    "print(\"Defining the optimized RNN model...\")\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=max_sequence_length))\n",
    "rnn_model.add(GRU(32, return_sequences=True))\n",
    "rnn_model.add(GRU(32))\n",
    "rnn_model.add(Dense(16, activation='relu'))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the RNN model\n",
    "print(\"Compiling the RNN model...\")\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Implement early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the RNN model\n",
    "print(\"Training the RNN model...\")\n",
    "rnn_model.fit(train_dataset, epochs=10, validation_data=test_dataset, callbacks=[early_stopping])\n",
    "print(\"RNN model training completed.\")\n",
    "\n",
    "# Evaluate the RNN model\n",
    "print(\"Evaluating the RNN model...\")\n",
    "loss_rnn, accuracy_rnn = rnn_model.evaluate(test_dataset)\n",
    "y_pred_rnn_probs = rnn_model.predict(test_dataset)\n",
    "y_pred_rnn = (y_pred_rnn_probs > 0.5).astype(int)\n",
    "\n",
    "# Collect predictions for calculating metrics\n",
    "y_test_array = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_pred_rnn_array = np.concatenate(y_pred_rnn, axis=0)\n",
    "\n",
    "precision_rnn = precision_score(y_test_array, y_pred_rnn_array)\n",
    "recall_rnn = recall_score(y_test_array, y_pred_rnn_array)\n",
    "f1_rnn = f1_score(y_test_array, y_pred_rnn_array)\n",
    "conf_matrix_rnn = confusion_matrix(y_test_array, y_pred_rnn_array)\n",
    "\n",
    "print(f'RNN Accuracy: {accuracy_rnn}')\n",
    "print(f'RNN Precision: {precision_rnn}')\n",
    "print(f'RNN Recall: {recall_rnn}')\n",
    "print(f'RNN F1 Score: {f1_rnn}')\n",
    "print('RNN Confusion Matrix:')\n",
    "print(conf_matrix_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49a70e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T06:13:53.477050Z",
     "iopub.status.busy": "2024-06-12T06:13:53.476617Z",
     "iopub.status.idle": "2024-06-12T06:13:53.483589Z",
     "shell.execute_reply": "2024-06-12T06:13:53.481980Z"
    },
    "papermill": {
     "duration": 0.215992,
     "end_time": "2024-06-12T06:13:53.486140",
     "exception": false,
     "start_time": "2024-06-12T06:13:53.270148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the sample test messages CSV file\n",
    "# print(\"Loading sample test messages CSV file...\")\n",
    "# test_file_path = '/kaggle/input/dsafgdf/corrected_messages.csv'  # Adjust the path as necessary\n",
    "# test_data = pd.read_csv(test_file_path)\n",
    "# print(\"Sample test messages loaded.\")\n",
    "\n",
    "# # Convert test messages to TF-IDF features\n",
    "# print(\"Converting test messages to TF-IDF features...\")\n",
    "# test_messages_tfidf = vectorizer.transform(test_data['message'])\n",
    "# print(\"Conversion completed.\")\n",
    "\n",
    "# # Predict using the Random Forest model\n",
    "# print(\"Predicting using the Random Forest model...\")\n",
    "# test_predictions_rf = rf_model.predict(test_messages_tfidf)\n",
    "# print(\"Random Forest Predictions:\", test_predictions_rf)\n",
    "\n",
    "# # Predict using the Deep Learning model\n",
    "# print(\"Predicting using the Deep Learning model...\")\n",
    "# test_predictions_dl = dl_model.predict(test_messages_tfidf.toarray())\n",
    "# test_predictions_dl = (test_predictions_dl > 0.5).astype(int)  # Convert probabilities to binary class\n",
    "# print(\"Neural Network Predictions:\", test_predictions_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5191764,
     "sourceId": 8664416,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5193783,
     "sourceId": 8667041,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 495.53125,
   "end_time": "2024-06-12T06:13:56.529765",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-12T06:05:40.998515",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
